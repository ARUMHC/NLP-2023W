{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.predict import MakePrediction\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../Project/data/news/314309.json','r') as f:\n",
    "    d = json.load(f)\n",
    "s = d['text']\n",
    "sentences = []\n",
    "for sen in s.split('\\n'):\n",
    "    if len(sen) > 0:\n",
    "        for ss in sen.split('.'):\n",
    "            if len(ss) > 0 and ss != '***':\n",
    "                sentences.append(ss.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SloNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './data/models/sloberta-2.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = MakePrediction(model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CamembertForTokenClassification(\n",
       "  (roberta): CamembertModel(\n",
       "    (embeddings): CamembertEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): CamembertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CamembertTokenizer(name_or_path='./data/models/sloberta-2.0', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>', 'additional_special_tokens': ['<s>NOTUSED', '</s>NOTUSED']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>NOTUSED\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>NOTUSED\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"<unk>NOTUSED\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32004: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.token_classification.TokenClassificationPipeline at 0x19d3ab01af0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.ner_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Drugouvrščena Švedinja Magdalena Forsberg je na 15-kilometrski razdalji za 26-letno Tržičanko zaostala za 27,5 sekunde, Ukrajinka Olena Petrova pa 35,5 sekunde'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = predictor.get_ners(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'O', 'score': 0.99907553, 'word': 'Drugo'},\n",
       " {'entity': 'O', 'score': 0.99902606, 'word': 'uvr'},\n",
       " {'entity': 'O', 'score': 0.99896324, 'word': 'ščena'},\n",
       " {'entity': 'B-per', 'score': 0.9764233, 'word': 'Š'},\n",
       " {'entity': 'B-per', 'score': 0.9768049, 'word': 'vedi'},\n",
       " {'entity': 'B-per', 'score': 0.97606146, 'word': 'nja'},\n",
       " {'entity': 'B-per', 'score': 0.97869724, 'word': 'Mag'},\n",
       " {'entity': 'B-per', 'score': 0.97902566, 'word': 'da'},\n",
       " {'entity': 'B-per', 'score': 0.9790486, 'word': 'lena'},\n",
       " {'entity': 'I-per', 'score': 0.97561556, 'word': 'For'},\n",
       " {'entity': 'I-per', 'score': 0.9753395, 'word': 's'},\n",
       " {'entity': 'I-per', 'score': 0.97548246, 'word': 'berg'},\n",
       " {'entity': 'O', 'score': 0.999054, 'word': 'je'},\n",
       " {'entity': 'O', 'score': 0.99915504, 'word': 'na'},\n",
       " {'entity': 'O', 'score': 0.9991658, 'word': '15-'},\n",
       " {'entity': 'O', 'score': 0.99917006, 'word': 'kilo'},\n",
       " {'entity': 'O', 'score': 0.99916875, 'word': 'metrski'},\n",
       " {'entity': 'O', 'score': 0.9991585, 'word': 'razdalji'},\n",
       " {'entity': 'O', 'score': 0.99911755, 'word': 'za'},\n",
       " {'entity': 'O', 'score': 0.99893457, 'word': '26-'},\n",
       " {'entity': 'O', 'score': 0.9980933, 'word': 'letno'},\n",
       " {'entity': 'B-per', 'score': 0.97779137, 'word': 'Trži'},\n",
       " {'entity': 'B-per', 'score': 0.9788408, 'word': 'čan'},\n",
       " {'entity': 'B-per', 'score': 0.978646, 'word': 'ko'},\n",
       " {'entity': 'O', 'score': 0.99913305, 'word': 'zaosta'},\n",
       " {'entity': 'O', 'score': 0.99912196, 'word': 'la'},\n",
       " {'entity': 'O', 'score': 0.9991399, 'word': 'za'},\n",
       " {'entity': 'O', 'score': 0.9991386, 'word': '27,'},\n",
       " {'entity': 'O', 'score': 0.9991229, 'word': '5'},\n",
       " {'entity': 'O', 'score': 0.9991153, 'word': 'sekunde'},\n",
       " {'entity': 'O', 'score': 0.9990946, 'word': ','},\n",
       " {'entity': 'B-per', 'score': 0.9732978, 'word': 'Ukraji'},\n",
       " {'entity': 'B-per', 'score': 0.97353214, 'word': 'nka'},\n",
       " {'entity': 'B-per', 'score': 0.977431, 'word': 'O'},\n",
       " {'entity': 'B-per', 'score': 0.975934, 'word': 'lena'},\n",
       " {'entity': 'I-per', 'score': 0.97383875, 'word': 'Petro'},\n",
       " {'entity': 'I-per', 'score': 0.9733469, 'word': 'va'},\n",
       " {'entity': 'O', 'score': 0.9991192, 'word': 'pa'},\n",
       " {'entity': 'O', 'score': 0.9991351, 'word': '35,'},\n",
       " {'entity': 'O', 'score': 0.9991013, 'word': '5'},\n",
       " {'entity': 'O', 'score': 0.9991014, 'word': 'sekunde'}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLOBERTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EMBEDDIA/sloberta\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 520/520 [00:00<00:00, 86.7kB/s]\n",
      "c:\\Users\\ILLIA\\anaconda3\\envs\\sloner10\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ILLIA\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "pytorch_model.bin: 100%|██████████| 443M/443M [00:12<00:00, 34.1MB/s] \n",
      "Some weights of CamembertModel were not initialized from the model checkpoint at EMBEDDIA/sloberta and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "tokenizer_config.json: 100%|██████████| 506/506 [00:00<00:00, 36.2kB/s]\n",
      "sentencepiece.bpe.model: 100%|██████████| 800k/800k [00:00<00:00, 1.74MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.65M/1.65M [00:00<00:00, 2.95MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 298/298 [00:00<00:00, 27.3kB/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(sentences[1], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    5,  8488, 16779,  5716,   384,  4322,    99, 14280,    55,  6710,\n",
       "          5695, 31781,  8688,    38,    28, 17391, 26508, 27820, 19896,    37,\n",
       "         29702,  4856, 21080,  3056,    20,  6830,    29,    37, 12151, 31818,\n",
       "         13452, 31791,  8697,  1838,   183,  6710,  7497,    36,    82, 14726,\n",
       "         31818, 13452,     6]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1691, -0.0264, -0.0332,  ...,  0.0883,  0.0967, -0.0622],\n",
       "         [ 0.0885,  0.1627,  0.1476,  ...,  0.1812, -0.2923,  0.4158],\n",
       "         [ 0.1389, -0.0202, -0.3754,  ...,  0.0367, -0.1503,  0.3582],\n",
       "         ...,\n",
       "         [-0.0079, -0.0023, -0.0225,  ...,  0.0154, -0.0357,  0.2704],\n",
       "         [ 0.0385, -0.1620, -0.2216,  ...,  0.0038, -0.1310,  0.0827],\n",
       "         [ 0.0553,  0.0106, -0.0426,  ...,  0.0703,  0.0527, -0.4959]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 1.3200e-01, -7.7165e-02,  7.7968e-02, -1.8721e-01, -1.1282e-01,\n",
       "          4.8820e-03, -3.4799e-02,  8.6005e-02,  2.7373e-01,  2.1589e-01,\n",
       "          1.6254e-02,  2.1581e-01, -1.0911e-02, -2.9335e-01, -7.0942e-02,\n",
       "          5.7695e-02,  1.1130e-01,  1.0329e-01,  1.0227e-01, -2.5548e-01,\n",
       "         -1.5508e-02,  2.6226e-01,  5.7149e-02, -1.3356e-01,  3.0792e-02,\n",
       "         -4.5911e-02, -5.4817e-02, -1.9255e-01, -6.4219e-02,  4.1139e-02,\n",
       "         -1.8316e-03,  4.6563e-04,  2.6478e-01, -3.3693e-02, -2.5967e-01,\n",
       "          1.0562e-01, -1.8342e-01,  8.6354e-02,  7.3557e-02, -1.9535e-02,\n",
       "         -4.6661e-02,  7.5694e-03, -1.2036e-01, -1.1341e-01,  1.8885e-02,\n",
       "          3.8554e-02,  1.6919e-01,  4.7120e-02, -3.3396e-02,  1.9268e-01,\n",
       "          3.6193e-03, -1.2435e-01,  1.3098e-01,  2.4627e-01,  8.4661e-02,\n",
       "         -3.4382e-01,  2.7184e-01,  1.4291e-01, -2.3352e-01,  2.9682e-02,\n",
       "          2.5772e-02, -1.1052e-01, -4.3716e-02,  1.4890e-01, -7.5830e-02,\n",
       "         -3.3675e-02, -1.1298e-01, -2.4677e-01,  1.2027e-01, -2.8167e-01,\n",
       "         -4.7060e-03, -2.6185e-01,  9.3967e-02, -2.2872e-02,  2.5540e-02,\n",
       "         -1.0435e-01,  3.8979e-02,  3.1667e-01, -3.0498e-01, -3.5329e-01,\n",
       "          3.5455e-01,  1.3295e-02,  2.2700e-01,  3.1416e-01,  6.4991e-02,\n",
       "         -1.9896e-01,  5.4254e-04, -4.2751e-02,  6.6149e-03, -1.0244e-01,\n",
       "         -5.1518e-02,  1.4768e-01, -6.5858e-02,  2.2027e-01,  9.3962e-02,\n",
       "          1.9248e-01, -1.7775e-01, -1.7951e-01,  6.1617e-02, -1.5774e-02,\n",
       "         -1.2077e-01, -8.8448e-02, -1.2951e-01, -1.4769e-01, -2.7666e-01,\n",
       "         -5.1340e-03, -2.0904e-02, -2.0692e-01, -1.0593e-01, -6.5467e-02,\n",
       "         -2.6445e-01,  2.0975e-01,  5.4544e-02,  3.9414e-02, -2.3126e-03,\n",
       "          2.4136e-02,  8.2008e-02, -1.8185e-01, -2.4789e-01,  2.2325e-01,\n",
       "         -8.7539e-02,  5.4885e-02,  4.2821e-01, -2.2058e-02, -3.4910e-01,\n",
       "         -2.2225e-01,  4.7251e-02, -2.2438e-01, -1.0664e-01,  1.2513e-01,\n",
       "         -7.9868e-02, -1.5622e-01, -6.6686e-02,  2.0346e-02, -2.2668e-01,\n",
       "         -2.2082e-01,  1.2383e-01, -2.4665e-01, -1.9037e-01,  2.1244e-01,\n",
       "         -3.2082e-02,  3.0265e-01,  1.1216e-02,  1.2529e-01, -2.2752e-01,\n",
       "          2.9019e-02,  9.5178e-02,  1.7341e-01,  1.1313e-01,  3.6704e-01,\n",
       "          9.5289e-02,  2.6601e-01, -7.8509e-02,  4.3147e-03,  1.0863e-01,\n",
       "          2.1203e-03, -4.6406e-02, -3.5464e-02, -3.6161e-01,  1.6475e-01,\n",
       "         -1.6311e-02, -2.6348e-01,  1.0447e-01,  8.6174e-02,  2.4484e-01,\n",
       "          3.6142e-01,  1.3336e-01,  2.9259e-01, -3.5938e-02,  2.2317e-01,\n",
       "          1.4641e-01, -4.2629e-02,  1.7542e-01, -1.6286e-01,  1.5073e-01,\n",
       "          6.7069e-02, -3.3374e-01, -6.8466e-02,  2.8693e-01, -2.7720e-01,\n",
       "          2.0776e-02, -1.2432e-01, -3.4701e-01,  2.4440e-02, -2.5939e-01,\n",
       "         -6.3930e-02, -1.2431e-01,  9.7000e-02,  1.6411e-01, -8.4483e-02,\n",
       "         -1.8850e-01, -2.0091e-01, -8.3810e-02,  1.0449e-01,  5.6628e-01,\n",
       "          9.3760e-02, -1.2723e-01, -1.9212e-01, -2.2958e-01,  1.0400e-01,\n",
       "         -3.3346e-01, -1.9855e-02, -7.5010e-02, -2.0541e-01,  8.2733e-02,\n",
       "          2.1054e-01, -3.5446e-01,  1.9207e-02, -4.1695e-01,  1.2659e-01,\n",
       "          2.4667e-02,  2.7629e-01, -1.6082e-01,  2.4935e-01,  1.7387e-01,\n",
       "          8.9835e-02, -8.3840e-03,  2.4648e-01, -7.2324e-02,  2.0867e-01,\n",
       "          1.2265e-01, -4.0723e-01, -3.5477e-01,  5.0037e-02,  8.1914e-02,\n",
       "         -1.5326e-01,  1.0871e-01,  2.5228e-01, -2.9689e-01,  1.5695e-01,\n",
       "          1.4143e-02,  2.6799e-01,  2.4629e-01, -3.4680e-01, -9.6659e-02,\n",
       "          6.8428e-02, -1.4487e-01, -1.4692e-01, -2.9331e-02,  7.9360e-02,\n",
       "         -1.0819e-01, -1.7167e-01, -3.0464e-01, -5.9651e-02, -1.1260e-01,\n",
       "          1.8951e-02, -1.9708e-01,  2.9755e-01,  7.9735e-02, -3.6478e-02,\n",
       "          8.6513e-02,  1.3331e-01, -5.0250e-01,  2.1306e-01,  5.3327e-03,\n",
       "          6.9605e-02,  3.1766e-01, -1.0751e-01, -6.0127e-02,  3.6124e-01,\n",
       "          1.4119e-01, -7.6955e-03, -2.9268e-01, -2.7271e-01, -1.3835e-01,\n",
       "          1.8387e-01,  1.9390e-01,  2.1909e-01, -2.1773e-01, -3.1025e-01,\n",
       "          2.0555e-01, -1.9061e-02,  2.7861e-01,  2.1086e-02, -2.4781e-01,\n",
       "          2.1759e-01,  6.7590e-02, -3.0831e-01,  1.1557e-01, -2.6755e-02,\n",
       "          4.2751e-02,  1.2470e-01, -6.5527e-02, -1.3390e-01, -1.2756e-02,\n",
       "          3.3339e-02,  1.3183e-01, -3.3015e-02, -2.0248e-01,  2.7567e-01,\n",
       "         -1.2260e-01,  2.2032e-01,  2.9110e-01, -5.2008e-02,  1.6354e-01,\n",
       "          6.1939e-03, -1.5564e-01,  1.9334e-01, -2.0252e-01, -7.2066e-02,\n",
       "         -1.3632e-01, -5.6779e-02,  8.9019e-02, -4.0620e-02, -2.3701e-01,\n",
       "          6.9255e-02,  7.3374e-02,  2.7499e-01, -3.6534e-01, -7.9820e-02,\n",
       "          1.1940e-01,  2.4616e-01,  1.3450e-02,  3.7685e-01, -9.3568e-02,\n",
       "         -4.8508e-03, -1.6939e-01,  4.3690e-01, -2.4553e-02,  1.6967e-01,\n",
       "          9.3768e-02, -1.9311e-02, -1.3199e-01,  4.7564e-01,  5.8337e-02,\n",
       "          1.8231e-01,  3.1662e-01,  1.3690e-01,  4.1776e-02,  6.8719e-02,\n",
       "          1.8443e-01,  1.6321e-01,  1.5013e-01,  4.8806e-01,  2.5256e-01,\n",
       "         -1.9450e-01,  1.4590e-01,  6.6309e-02,  7.6608e-02, -2.3593e-01,\n",
       "          1.3948e-01,  1.9184e-01,  2.7677e-02, -3.8606e-02,  2.2751e-01,\n",
       "          3.4334e-01, -1.6164e-02,  1.8444e-02, -1.7466e-01, -1.7849e-01,\n",
       "         -3.6052e-02, -2.7711e-01,  4.0156e-01,  3.6639e-01,  8.8693e-02,\n",
       "          1.0928e-01, -4.3435e-03,  2.4954e-01, -3.3039e-02,  8.7187e-03,\n",
       "         -7.0638e-02,  1.9024e-01, -2.3562e-02,  2.8721e-02, -1.4396e-01,\n",
       "         -2.3735e-01, -2.4638e-01,  1.8578e-01, -8.8772e-02, -4.3250e-02,\n",
       "          1.5605e-01,  1.6604e-02,  5.6885e-02,  1.8304e-01, -6.5134e-03,\n",
       "          2.4114e-03,  7.9927e-02,  1.1073e-01, -1.2826e-02,  2.7455e-01,\n",
       "          6.9882e-02, -1.8257e-01,  3.4889e-02,  7.1816e-02, -1.4686e-02,\n",
       "         -2.5618e-01, -4.0487e-01,  5.8786e-02, -1.5409e-01, -2.2966e-02,\n",
       "          7.8401e-03, -7.3799e-02, -2.2087e-02,  9.7825e-02,  2.2657e-03,\n",
       "          1.2342e-01, -1.3091e-03, -7.7512e-02, -1.7560e-01,  2.4831e-01,\n",
       "         -1.6015e-01, -1.9874e-01,  5.1712e-02,  1.2443e-02, -3.1732e-01,\n",
       "         -1.6382e-02,  4.0817e-02, -1.1265e-03, -7.1549e-02, -1.3892e-01,\n",
       "          1.9215e-01,  3.9954e-02, -1.3542e-01,  9.9217e-02, -1.5705e-01,\n",
       "         -1.4253e-01,  2.7649e-02,  1.6187e-01,  1.1414e-01,  2.3698e-01,\n",
       "         -1.0817e-01,  5.8583e-02, -1.8273e-01,  2.7571e-01,  2.0763e-01,\n",
       "         -1.2315e-01,  6.2563e-02,  2.9351e-01, -1.7770e-01,  8.5909e-02,\n",
       "         -4.8932e-02, -3.6364e-02, -4.3889e-02, -7.5137e-02, -3.1051e-01,\n",
       "          6.7024e-02, -1.6500e-01, -1.0227e-01, -9.8127e-02, -3.3060e-02,\n",
       "          6.0749e-02,  1.7356e-02, -2.5652e-01,  1.8816e-01, -2.3090e-01,\n",
       "         -1.4360e-01,  2.3241e-02, -1.8564e-01,  1.4952e-01, -3.7026e-01,\n",
       "         -2.6146e-02, -1.0398e-01,  1.2959e-01,  1.2187e-01, -3.3168e-01,\n",
       "         -9.5066e-03, -2.4342e-01, -6.7135e-02, -8.2018e-02, -9.4599e-02,\n",
       "          1.3568e-01, -2.0241e-02,  1.0973e-01,  9.9004e-02,  2.5514e-01,\n",
       "          8.9906e-02, -2.1630e-02, -4.3273e-01,  7.0525e-02,  1.3108e-01,\n",
       "          1.8558e-01,  1.3900e-01, -2.6115e-01,  2.9653e-02,  3.8471e-02,\n",
       "          3.4981e-01,  7.8597e-04, -1.5078e-02,  9.8807e-02, -9.5586e-02,\n",
       "         -3.1080e-01,  2.6402e-01, -2.3880e-01,  2.9942e-01,  4.2628e-02,\n",
       "         -5.7336e-02, -2.6008e-01, -9.2832e-02, -3.3657e-01,  2.1146e-01,\n",
       "          1.4540e-01, -6.8311e-02, -1.2587e-01, -1.1451e-01,  2.3271e-01,\n",
       "          1.4847e-01,  1.0276e-01, -1.4642e-01, -2.8628e-01,  1.3966e-01,\n",
       "          9.6955e-02,  3.1083e-02, -1.0060e-01, -6.5644e-03,  2.3541e-01,\n",
       "          1.5208e-01,  1.8889e-01,  3.5793e-01, -3.0660e-01, -2.3515e-02,\n",
       "         -1.6072e-01, -1.5174e-02, -5.7120e-03, -2.3257e-01, -4.3635e-01,\n",
       "         -3.4281e-01, -1.6317e-02, -1.1763e-01, -1.3547e-01,  1.3673e-01,\n",
       "          1.1497e-01,  2.9849e-01, -4.0735e-01, -2.7679e-01, -3.6122e-02,\n",
       "          1.5353e-02, -1.0317e-01, -1.1259e-01,  6.6132e-02,  5.7870e-02,\n",
       "         -7.3223e-02, -1.5390e-01, -2.9848e-02, -4.3744e-02, -2.2807e-01,\n",
       "          3.4005e-01, -4.9451e-01, -5.2585e-03, -1.6501e-01, -1.4847e-02,\n",
       "          4.1448e-02,  1.2419e-01, -3.5890e-02,  3.9839e-01, -2.5832e-02,\n",
       "          1.1558e-01,  2.2280e-01,  2.4473e-01, -7.3969e-02, -9.8330e-02,\n",
       "         -2.1488e-02,  1.5866e-01, -1.0465e-01, -2.4880e-01,  7.9386e-03,\n",
       "         -3.7024e-02,  1.0446e-01, -4.0159e-02,  2.1619e-02,  1.1342e-04,\n",
       "          3.6759e-02, -9.1151e-02,  1.2707e-01, -1.1743e-02, -3.9939e-02,\n",
       "         -2.2834e-01, -8.7172e-02,  9.8172e-02, -2.0750e-01,  3.2027e-01,\n",
       "         -1.2320e-01, -1.5154e-01, -1.8377e-01, -1.0134e-01, -7.9063e-02,\n",
       "          1.3874e-01,  4.9842e-01,  6.1307e-02,  1.7153e-02, -2.8357e-01,\n",
       "          1.9297e-01, -1.7765e-01, -2.3905e-01, -7.7838e-02,  6.1233e-02,\n",
       "         -1.3727e-01, -4.9400e-02,  9.7828e-02, -4.7144e-02, -2.0920e-01,\n",
       "         -1.9791e-01,  6.6724e-02,  1.1606e-01, -1.6951e-01,  2.0406e-01,\n",
       "          3.6395e-01, -3.3295e-01,  6.3393e-02,  1.5288e-01, -1.9766e-01,\n",
       "          1.9076e-01,  2.8697e-01, -1.6158e-01,  7.1609e-02,  1.7699e-01,\n",
       "          1.6202e-01,  3.5022e-01,  1.6004e-01, -6.0322e-03, -1.0228e-01,\n",
       "         -2.7580e-02,  6.5493e-02, -1.2324e-01,  3.0210e-01, -3.1091e-01,\n",
       "          1.2952e-01,  3.1699e-01,  2.7136e-01, -5.3835e-02, -2.7154e-03,\n",
       "          2.7867e-01, -1.4527e-01, -1.1999e-02,  2.9580e-01,  1.3591e-01,\n",
       "         -3.2650e-01,  1.5064e-01,  5.1332e-02,  1.0209e-01, -4.3570e-01,\n",
       "         -1.0443e-01,  1.1219e-01,  1.0827e-01,  1.9762e-01, -2.6164e-01,\n",
       "          1.3933e-01,  7.5717e-02,  4.2842e-02,  1.8950e-01, -1.8610e-01,\n",
       "         -3.0490e-01,  7.9577e-02, -6.8355e-02, -2.8923e-01, -8.5958e-02,\n",
       "         -2.1207e-01,  3.9978e-01,  1.5804e-01, -2.5941e-01,  8.3956e-02,\n",
       "         -4.0374e-03, -1.8061e-01, -9.3347e-03, -1.3512e-01, -4.5503e-02,\n",
       "         -1.4772e-01, -2.6753e-01,  2.8031e-01, -1.0550e-01,  1.2034e-01,\n",
       "          2.2657e-01, -1.3519e-01, -1.9557e-01, -1.0915e-01,  3.3366e-02,\n",
       "         -1.5275e-01, -1.7719e-01, -6.9056e-02,  3.2393e-02, -2.4768e-01,\n",
       "          9.8258e-02,  6.2185e-02,  5.9465e-02,  1.6989e-02, -3.3183e-01,\n",
       "          8.9880e-02, -1.2026e-01,  7.1323e-02,  1.3156e-01,  8.7475e-02,\n",
       "          1.9857e-01,  4.1285e-02, -2.0007e-01,  7.6725e-02,  3.0031e-01,\n",
       "          1.2877e-01,  5.6785e-02,  8.5310e-03,  1.0386e-01,  8.4952e-02,\n",
       "         -5.3127e-02,  2.2224e-01,  2.9058e-01,  1.8659e-01, -1.3789e-01,\n",
       "         -1.1105e-01,  4.2303e-02,  7.2218e-02, -1.7320e-02, -9.4830e-02,\n",
       "          1.1986e-01, -8.6606e-03,  2.4864e-02, -9.8462e-02,  1.7368e-02,\n",
       "          1.5331e-01, -7.2515e-03, -2.7401e-01, -1.8649e-01, -2.0722e-01,\n",
       "         -8.8603e-02,  2.4508e-01,  1.5627e-01,  4.2616e-01, -9.2184e-02,\n",
       "          7.0315e-02,  1.6527e-01,  1.2734e-01, -1.8691e-01,  2.7633e-01,\n",
       "         -3.7937e-01, -9.4026e-03, -9.2944e-02,  1.4882e-01, -1.0443e-01,\n",
       "          2.7666e-01,  1.9776e-02, -2.1051e-01,  7.6041e-02, -2.4295e-01,\n",
       "          2.8518e-02, -2.0722e-01,  1.5025e-01, -2.4711e-01, -1.9598e-01,\n",
       "          2.1339e-01,  1.9080e-01, -6.0210e-02,  9.4465e-02,  1.2905e-01,\n",
       "         -5.1283e-01, -2.4401e-01, -1.6604e-02, -1.6667e-01, -3.5512e-02,\n",
       "         -3.0660e-02, -2.2963e-01, -6.0544e-02,  7.1600e-02, -4.5347e-03,\n",
       "         -1.3600e-02, -5.7339e-02,  2.1725e-01,  4.0956e-02, -1.4732e-01,\n",
       "          9.1190e-02, -4.2415e-02,  1.2042e-01,  3.3643e-01, -7.5078e-03,\n",
       "          4.4319e-01, -1.0007e-01,  2.9038e-01,  1.6994e-01, -4.3602e-02,\n",
       "         -4.5699e-02, -9.3230e-02,  4.3140e-02]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sloner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
