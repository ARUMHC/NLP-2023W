{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "The notebook explores step-by-step process of PLM-based phrase embedding calculcation that is presented in 'plm_emb.py' script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import copy\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "from nltk import sent_tokenize\n",
    "from functools import reduce\n",
    "import mmap\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertModel were not initialized from the model checkpoint at EMBEDDIA/sloberta and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "model_name = \"EMBEDDIA/sloberta\" \n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\ILLIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\ILLIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\ILLIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\ILLIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\ILLIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\ILLIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\ILLIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\ILLIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\ILLIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\ILLIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\ILLIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\ILLIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\ILLIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\ILLIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\ILLIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\ILLIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\ILLIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\ILLIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\ILLIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\ILLIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\ILLIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\ILLIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Project/data/news/314309.json','r') as f:\n",
    "    d = json.load(f)\n",
    "s = d['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(s.strip().lower().replace(' .', '.'),language='slovene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_toks = [tok for tok in sentences[1].replace('.', ' .').split(' ') if tok != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_enc = tokenizer([tok.replace('_', ' ') for tok in sent_toks], add_special_tokens=False)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1578, 16779, 5716],\n",
       " [98, 4322, 99],\n",
       " [7006, 55, 6710],\n",
       " [1586, 31781, 8688],\n",
       " [38],\n",
       " [28],\n",
       " [17391, 26508, 27820],\n",
       " [19896],\n",
       " [37],\n",
       " [29702, 4856],\n",
       " [7187, 3056, 20],\n",
       " [6830, 29],\n",
       " [37],\n",
       " [12151, 31818],\n",
       " [13452, 31791],\n",
       " [19019, 1838],\n",
       " [23, 6710],\n",
       " [353, 213, 36],\n",
       " [82],\n",
       " [14726, 31818],\n",
       " [13452],\n",
       " [1666]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 4,\n",
       " 7,\n",
       " 10,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 22,\n",
       " 25,\n",
       " 27,\n",
       " 28,\n",
       " 30,\n",
       " 32,\n",
       " 34,\n",
       " 36,\n",
       " 39,\n",
       " 40,\n",
       " 42,\n",
       " 43,\n",
       " 44]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1]+(np.cumsum([len(ids) for ids in tok_enc])+1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [1] + (np.cumsum([len(ids) for ids in tok_enc]) + 1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1578, 16779, 5716],\n",
       " [98, 4322, 99],\n",
       " [7006, 55, 6710],\n",
       " [1586, 31781, 8688],\n",
       " [38],\n",
       " [28],\n",
       " [17391, 26508, 27820],\n",
       " [19896],\n",
       " [37],\n",
       " [29702, 4856],\n",
       " [7187, 3056, 20],\n",
       " [6830, 29],\n",
       " [37],\n",
       " [12151, 31818],\n",
       " [13452, 31791],\n",
       " [19019, 1838],\n",
       " [23, 6710],\n",
       " [353, 213, 36],\n",
       " [82],\n",
       " [14726, 31818],\n",
       " [13452],\n",
       " [1666]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_ids = [tokenizer.cls_token_id] + reduce(lambda x, y: x + y, tok_enc, []) + [tokenizer.sep_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5,\n",
       " 1578,\n",
       " 16779,\n",
       " 5716,\n",
       " 98,\n",
       " 4322,\n",
       " 99,\n",
       " 7006,\n",
       " 55,\n",
       " 6710,\n",
       " 1586,\n",
       " 31781,\n",
       " 8688,\n",
       " 38,\n",
       " 28,\n",
       " 17391,\n",
       " 26508,\n",
       " 27820,\n",
       " 19896,\n",
       " 37,\n",
       " 29702,\n",
       " 4856,\n",
       " 7187,\n",
       " 3056,\n",
       " 20,\n",
       " 6830,\n",
       " 29,\n",
       " 37,\n",
       " 12151,\n",
       " 31818,\n",
       " 13452,\n",
       " 31791,\n",
       " 19019,\n",
       " 1838,\n",
       " 23,\n",
       " 6710,\n",
       " 353,\n",
       " 213,\n",
       " 36,\n",
       " 82,\n",
       " 14726,\n",
       " 31818,\n",
       " 13452,\n",
       " 1666,\n",
       " 6]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flat_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab={}\n",
    "inv_vocab={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tok in sent_toks:\n",
    "    if tok not in vocab:\n",
    "        vocab[tok] = len(vocab)\n",
    "        inv_vocab[vocab[tok]] = tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ses=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ses.append((flat_ids, [vocab[tok] for tok in sent_toks], indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([5,\n",
       "   1578,\n",
       "   16779,\n",
       "   5716,\n",
       "   98,\n",
       "   4322,\n",
       "   99,\n",
       "   7006,\n",
       "   55,\n",
       "   6710,\n",
       "   1586,\n",
       "   31781,\n",
       "   8688,\n",
       "   38,\n",
       "   28,\n",
       "   17391,\n",
       "   26508,\n",
       "   27820,\n",
       "   19896,\n",
       "   37,\n",
       "   29702,\n",
       "   4856,\n",
       "   7187,\n",
       "   3056,\n",
       "   20,\n",
       "   6830,\n",
       "   29,\n",
       "   37,\n",
       "   12151,\n",
       "   31818,\n",
       "   13452,\n",
       "   31791,\n",
       "   19019,\n",
       "   1838,\n",
       "   23,\n",
       "   6710,\n",
       "   353,\n",
       "   213,\n",
       "   36,\n",
       "   82,\n",
       "   14726,\n",
       "   31818,\n",
       "   13452,\n",
       "   1666,\n",
       "   6],\n",
       "  [0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   8,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20],\n",
       "  [1,\n",
       "   4,\n",
       "   7,\n",
       "   10,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   22,\n",
       "   25,\n",
       "   27,\n",
       "   28,\n",
       "   30,\n",
       "   32,\n",
       "   34,\n",
       "   36,\n",
       "   39,\n",
       "   40,\n",
       "   42,\n",
       "   43,\n",
       "   44])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 721/721 [00:09<00:00, 76.18it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "inv_vocab = {}\n",
    "sentences = []\n",
    "src_dir = '../Project/data/news/'\n",
    "for file in tqdm(os.listdir(src_dir),total=len(os.listdir(src_dir))):\n",
    "    with open(os.path.join(src_dir,file),'r') as f:\n",
    "        d = json.load(f)\n",
    "    if 'text' in d:\n",
    "        s = d['text']\n",
    "        for sent in sent_tokenize(s.strip().lower().replace(' .', '.'),language='slovene'):\n",
    "            sent_toks = [tok for tok in sent.replace('.', ' .').split(' ') if tok != '']\n",
    "            tok_enc = tokenizer([tok.replace('_', ' ') for tok in sent_toks], add_special_tokens=False)['input_ids']\n",
    "            indices = [1] + (np.cumsum([len(ids) for ids in tok_enc]) + 1).tolist()\n",
    "            flat_ids = [tokenizer.cls_token_id] + reduce(lambda x, y: x + y, tok_enc, []) + [tokenizer.sep_token_id]\n",
    "            if len(flat_ids) > 512: continue\n",
    "            for tok in sent_toks:\n",
    "                if tok not in vocab:\n",
    "                    vocab[tok] = len(vocab)\n",
    "                    inv_vocab[vocab[tok]] = tok\n",
    "            sentences.append((flat_ids, [vocab[tok] for tok in sent_toks], indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "iterations = int(len(sentences)/batch_size) + (0 if len(sentences) % batch_size == 0 else 1)\n",
    "phrase_div = np.zeros((len(vocab), 1))\n",
    "phrase_emb = np.zeros((len(vocab), 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = 0 * batch_size\n",
    "end = min((0+1)*batch_size, len(sentences))\n",
    "start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ids = [ids for ids,_,_ in sentences[start:end]]\n",
    "batch_max_length = max(len(ids) for ids in batch_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "[5, 4269, 12000, 31791, 3245, 1666, 4700, 100, 97, 31808, 233, 844, 68, 72, 735, 4015, 9056, 100, 59, 144, 238, 31791, 224, 861, 31791, 9749, 238, 31808, 13, 26047, 4235, 1723, 34, 8834, 23, 6796, 195, 13, 9203, 1236, 31791, 13, 4704, 2615, 2192, 9799, 7375, 78, 28785, 282, 45, 227, 78, 72, 4382, 1849, 34, 31791, 67, 72, 12826, 18762, 4299, 33, 4205, 2667, 17943, 33, 366, 1593, 4302, 175, 10192, 3931, 34, 3999, 18, 5342, 11708, 31791, 3443, 12230, 4015, 10209, 33, 26075, 173, 5688, 5448, 28785, 6796, 13, 18282, 2840, 12131, 9713, 3999, 89, 5986, 1666, 6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(batch_ids)),print(batch_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 101])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = torch.tensor([ids + [0 for _ in range(batch_max_length - len(ids))] for ids in batch_ids]).long()\n",
    "masks = (ids != 0).long()\n",
    "ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    5, 17868,  1763,    68,   631,  3452, 15877,   270,  3999,    89,\n",
       "         5986,  8018,  1471,  3464, 28004,    33,    23, 14769,    79,   265,\n",
       "        23811,  3833, 12131,  1684,   283,   407,    18, 12799, 31840,  1320,\n",
       "        11769,  7375,  3700,  1236,  7752,    10, 31791,   157,    38, 26075,\n",
       "          173,  5688,  3952, 13044,   642,  1845,  1090, 31791,    67,  1319,\n",
       "         2665,  1666,     6,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1348e-01, -7.0428e-02, -3.2933e-02,  ..., -9.5881e-02,\n",
       "           8.2250e-02,  1.3673e-02],\n",
       "         [-4.5574e-01,  5.2130e-02, -5.3951e-01,  ..., -2.7226e-01,\n",
       "          -2.2225e-01, -6.6631e-01],\n",
       "         [-3.9055e-01,  3.9250e-02,  3.0773e-01,  ..., -2.7218e-01,\n",
       "          -2.0976e-01, -1.9855e-01],\n",
       "         ...,\n",
       "         [-2.9948e-01,  1.2349e-01,  9.0223e-02,  ..., -3.5650e-01,\n",
       "           2.6992e-01, -3.1598e-01],\n",
       "         [-1.5737e-01, -7.9548e-02,  1.3892e-01,  ..., -1.7102e-02,\n",
       "          -3.4802e-02, -3.3593e-01],\n",
       "         [-7.1166e-02,  8.8937e-02,  1.1284e-01,  ...,  2.2654e-01,\n",
       "           2.5839e-01, -4.9933e-01]],\n",
       "\n",
       "        [[ 1.7489e-01, -8.3358e-02, -1.0271e-02,  ..., -4.3773e-02,\n",
       "           2.7871e-02, -1.1671e-01],\n",
       "         [-5.7650e-02, -4.0602e-02,  2.5706e-01,  ..., -1.8942e-01,\n",
       "           2.2271e-01, -8.8724e-02],\n",
       "         [-8.7885e-02,  1.5177e-01,  5.4022e-02,  ..., -2.3902e-02,\n",
       "          -2.5731e-03, -1.2130e-01],\n",
       "         ...,\n",
       "         [ 1.8795e-01, -8.7092e-02,  1.0912e-01,  ...,  1.5315e-03,\n",
       "           5.2118e-03,  7.2304e-03],\n",
       "         [ 1.8617e-01, -8.7367e-02,  1.0617e-01,  ..., -1.3800e-03,\n",
       "           6.0407e-04,  7.2798e-03],\n",
       "         [ 1.8567e-01, -8.6447e-02,  9.0385e-02,  ..., -8.0949e-03,\n",
       "           1.9546e-03, -8.0241e-03]],\n",
       "\n",
       "        [[ 1.2481e-01, -8.7219e-02,  2.9032e-02,  ..., -2.8934e-02,\n",
       "           3.6540e-02, -1.1272e-01],\n",
       "         [ 3.1581e-01, -1.4864e-01,  1.3818e-01,  ...,  3.2146e-02,\n",
       "          -1.3953e-01,  4.6086e-01],\n",
       "         [ 2.6516e-01, -1.4430e-01,  1.3809e-01,  ...,  1.4818e-01,\n",
       "          -1.5601e-01,  3.8777e-01],\n",
       "         ...,\n",
       "         [ 1.3202e-01, -8.7209e-02,  1.3847e-01,  ...,  7.8388e-03,\n",
       "           1.0851e-02,  6.9421e-03],\n",
       "         [ 1.3160e-01, -8.7399e-02,  1.3768e-01,  ...,  7.1725e-03,\n",
       "           1.0071e-02,  6.6263e-03],\n",
       "         [ 1.3187e-01, -8.7159e-02,  1.3176e-01,  ...,  5.0696e-03,\n",
       "           1.1768e-02,  1.7639e-05]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.0538e-01, -1.1763e-01,  1.1299e-01,  ..., -6.1165e-03,\n",
       "           8.4741e-02, -1.4994e-01],\n",
       "         [-3.0515e-01, -3.0036e-02, -1.2903e-01,  ..., -5.6983e-02,\n",
       "          -4.7011e-02, -3.0907e-01],\n",
       "         [ 4.4096e-03,  4.5626e-02,  1.2732e-03,  ...,  2.2144e-01,\n",
       "           1.7277e-02, -2.4659e-01],\n",
       "         ...,\n",
       "         [ 9.8353e-02, -1.2098e-01,  2.0128e-01,  ...,  1.8328e-02,\n",
       "           5.9666e-02, -6.0546e-02],\n",
       "         [ 9.8798e-02, -1.2074e-01,  1.9884e-01,  ...,  1.7614e-02,\n",
       "           6.1081e-02, -6.3552e-02],\n",
       "         [ 9.9630e-02, -1.2026e-01,  1.9515e-01,  ...,  1.6634e-02,\n",
       "           6.3100e-02, -6.7978e-02]],\n",
       "\n",
       "        [[ 1.6885e-01, -8.9539e-02,  1.0194e-02,  ..., -2.2715e-02,\n",
       "           1.0446e-02, -1.6274e-01],\n",
       "         [ 5.2320e-02,  3.8780e-02, -1.2498e-01,  ..., -2.3715e-01,\n",
       "          -8.8951e-02, -3.1585e-02],\n",
       "         [ 2.4555e-01, -2.4434e-01, -1.3185e-01,  ...,  3.4100e-02,\n",
       "           1.1929e-01,  6.2294e-02],\n",
       "         ...,\n",
       "         [ 1.9707e-01, -9.1502e-02,  1.3927e-01,  ...,  2.9009e-02,\n",
       "          -9.7351e-03, -3.5115e-02],\n",
       "         [ 2.0305e-01, -9.4397e-02,  1.5824e-01,  ...,  4.5131e-02,\n",
       "           2.5394e-03, -2.3612e-02],\n",
       "         [ 2.0219e-01, -9.1952e-02,  1.5777e-01,  ...,  4.0082e-02,\n",
       "          -2.6091e-03, -2.1510e-02]],\n",
       "\n",
       "        [[ 2.0178e-01, -1.6093e-01,  8.3004e-02,  ...,  4.8170e-02,\n",
       "           3.9325e-02, -8.5897e-02],\n",
       "         [-2.0736e-01, -9.0754e-02, -1.0252e-01,  ..., -8.3359e-03,\n",
       "          -1.7366e-01, -3.1535e-01],\n",
       "         [ 8.9056e-03, -1.9583e-02, -3.7205e-01,  ..., -1.2892e-01,\n",
       "          -2.2864e-01, -1.3712e-01],\n",
       "         ...,\n",
       "         [ 2.0053e-01, -1.6164e-01,  1.8357e-01,  ...,  8.0520e-02,\n",
       "           3.0777e-03,  2.3171e-02],\n",
       "         [ 1.9949e-01, -1.6235e-01,  1.8471e-01,  ...,  8.0157e-02,\n",
       "           2.1168e-03,  2.4451e-02],\n",
       "         [ 1.9990e-01, -1.6197e-01,  1.8450e-01,  ...,  8.0386e-02,\n",
       "           2.1903e-03,  2.4404e-02]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_final_layer = model(ids, masks)[0]\n",
    "batch_final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 101, 768])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_final_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 17868, 1763, 68, 631, 3452, 15877, 270, 3999, 89, 5986, 8018, 1471, 3464, 28004, 33, 23, 14769, 79, 265, 23811, 3833, 12131, 1684, 283, 407, 18, 12799, 31840, 1320, 11769, 7375, 3700, 1236, 7752, 10, 31791, 157, 38, 26075, 173, 5688, 3952, 13044, 642, 1845, 1090, 31791, 67, 1319, 2665, 1666, 6]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56, 7, 57, 58, 59, 54, 55, 60, 61, 62, 34, 19, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 48, 77, 78, 79, 80, 81, 30, 82, 83, 2]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 4, 5, 6, 8, 10, 11, 13, 14, 15, 16, 17, 19, 20, 22, 25, 27, 29, 30, 31, 32, 33, 34, 37, 38, 39, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([101, 768]) 64 65\n",
      "torch.Size([101, 768]) 36 37\n",
      "torch.Size([101, 768]) 47 48\n",
      "torch.Size([101, 768]) 34 35\n",
      "torch.Size([101, 768]) 32 33\n",
      "torch.Size([101, 768]) 39 40\n",
      "torch.Size([101, 768]) 37 38\n",
      "torch.Size([101, 768]) 42 43\n",
      "torch.Size([101, 768]) 28 29\n",
      "torch.Size([101, 768]) 26 27\n",
      "torch.Size([101, 768]) 37 38\n",
      "torch.Size([101, 768]) 17 18\n",
      "torch.Size([101, 768]) 17 18\n",
      "torch.Size([101, 768]) 28 29\n",
      "torch.Size([101, 768]) 19 20\n",
      "torch.Size([101, 768]) 21 22\n",
      "torch.Size([101, 768]) 30 31\n",
      "torch.Size([101, 768]) 24 25\n",
      "torch.Size([101, 768]) 30 31\n",
      "torch.Size([101, 768]) 29 30\n",
      "torch.Size([101, 768]) 20 21\n",
      "torch.Size([101, 768]) 19 20\n",
      "torch.Size([101, 768]) 27 28\n",
      "torch.Size([101, 768]) 16 17\n",
      "torch.Size([101, 768]) 13 14\n",
      "torch.Size([101, 768]) 38 39\n",
      "torch.Size([101, 768]) 35 36\n",
      "torch.Size([101, 768]) 44 45\n",
      "torch.Size([101, 768]) 22 23\n",
      "torch.Size([101, 768]) 10 11\n",
      "torch.Size([101, 768]) 21 22\n",
      "torch.Size([101, 768]) 16 17\n"
     ]
    }
   ],
   "source": [
    "for final_layer, (_,sent_ids,indices) in zip(batch_final_layer, sentences[start:end]):\n",
    "    print(final_layer.shape, len(sent_ids), len(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [08:57<00:00,  9.61s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(iterations)):\n",
    "    start = i * batch_size\n",
    "    end = min((i+1)*batch_size, len(sentences))\n",
    "    batch_ids = [ids for ids,_,_ in sentences[start:end]]\n",
    "    batch_max_length = max(len(ids) for ids in batch_ids)\n",
    "    ids = torch.tensor([ids + [0 for _ in range(batch_max_length - len(ids))] for ids in batch_ids]).long()\n",
    "    masks = (ids != 0).long()\n",
    "    #ids = ids.to(device)\n",
    "    #masks = masks.to(device)\n",
    "    with torch.no_grad():\n",
    "        batch_final_layer = model(ids, masks)[0]\n",
    "    for final_layer, (_,sent_ids,indices) in zip(batch_final_layer, sentences[start:end]):\n",
    "        for idx in range(len(sent_ids)):\n",
    "            tok_id = sent_ids[idx]\n",
    "            phrase_emb[tok_id] += np.mean(final_layer[indices[idx]:indices[idx+1]].cpu().numpy(), axis=0)\n",
    "            phrase_div[tok_id] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ave_phrase_emb = phrase_emb / phrase_div\n",
    "\n",
    "kv = KeyedVectors(768)\n",
    "kv.add_vectors([inv_vocab[i] for i in range(len(vocab))], ave_phrase_emb)\n",
    "kv.save(f'example_sloberta.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13560"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sloner10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
